{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen: cannot load any more object with static TLS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-14e333c7f8f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtractseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAFQ_MultiCompCorrection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_significant_areas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtractseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetric_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtractseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtractseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tractseg/libs/plot_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtractseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_specific_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_dl_flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen: cannot load any more object with static TLS"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import math\n",
    "from decimal import Decimal\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tractseg.data import dataset_specific_utils\n",
    "from tractseg.libs.AFQ_MultiCompCorrection import AFQ_MultiCompCorrection\n",
    "from tractseg.libs.AFQ_MultiCompCorrection import get_significant_areas\n",
    "from tractseg.libs import metric_utils\n",
    "from tractseg.libs import plot_utils\n",
    "from tractseg.libs import tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import argparse\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from nibabel import trackvis\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tractseg.libs import tractometry\n",
    "from tractseg.data import dataset_specific_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/72 [00:00<?, ?it/s]\u001b[A\u001b[A/home/global/anaconda37/lib/python3.7/site-packages/ipykernel_launcher.py:33: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "/home/global/anaconda37/lib/python3.7/site-packages/ipykernel_launcher.py:34: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "\n",
      "\n",
      "  1%|▏         | 1/72 [00:26<30:49, 26.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 2/72 [00:43<27:19, 23.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 3/72 [00:55<23:06, 20.10s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "    results = []\n",
    "    tracking_dir='/NAS/dumbo/protocoles/CogPhenoPark/TractSeg/T84079/FOD_iFOD2_trackings/'\n",
    "    endings_dir='/NAS/dumbo/protocoles/CogPhenoPark/TractSeg/T84079/endings_segmentations/'\n",
    "    tracking_format='tck'\n",
    "    csv_file_out='/NAS/dumbo/protocoles/CogPhenoPark/TractSeg/T84079/L3_tractometry_72.csv'\n",
    "    scalar_image = ''\n",
    "    scalar_image = nib.load('/NAS/dumbo/protocoles/CogPhenoPark/TractSeg/T84079/fit_L3.nii.gz')\n",
    "    bundles = dataset_specific_utils.get_bundle_names(\"All\")[1:]    \n",
    "    NR_POINTS=102\n",
    "    DILATION = 2\n",
    "    for bundle in tqdm(bundles):\n",
    "        if False : #args.peak_length:\n",
    "            predicted_peaks = nib.load(join(args.TOM_dir, bundle + \".nii.gz\")).get_data()\n",
    "        else:\n",
    "            predicted_peaks = None\n",
    "        beginnings = nib.load(join(endings_dir, bundle + \"_b.nii.gz\"))\n",
    "\n",
    "        file_ending = \"trk\" if tracking_format == \"trk_legacy\" else tracking_format\n",
    "        trk_path = join(tracking_dir, bundle + \".\" + file_ending)\n",
    "\n",
    "        if not os.path.exists(trk_path):\n",
    "            print(\"WARNING: No tracking found for bundle {}. Returning zeros.\".format(bundle))\n",
    "            mean = np.zeros(NR_POINTS)\n",
    "            std = np.zeros(NR_POINTS)\n",
    "        else:\n",
    "            if tracking_format == \"trk_legacy\":\n",
    "                streams, hdr = trackvis.read(trk_path)\n",
    "                streamlines = [s[0] for s in streams]\n",
    "            else:\n",
    "                sl_file = nib.streamlines.load(trk_path)\n",
    "                streamlines = sl_file.streamlines\n",
    "\n",
    "            if len(streamlines) >= 5 :\n",
    "                mean, std = tractometry.evaluate_along_streamlines(np.nan_to_num(scalar_image.get_data()), streamlines,\n",
    "                                                               beginnings.get_data(), NR_POINTS, dilate=DILATION,\n",
    "                                                               predicted_peaks=predicted_peaks, affine=scalar_image.affine)\n",
    "            else:\n",
    "                print(\"WARNING: bundle {} contains less than 5 streamlines. Saving value 0 for this bundle.\".\n",
    "                      format(bundle))\n",
    "                mean = np.zeros(NR_POINTS)\n",
    "                std = np.zeros(NR_POINTS)\n",
    "\n",
    "        # Remove first and last segment as those tend to be more noisy\n",
    "        mean = mean[1:-1]\n",
    "        std = std[1:-1]\n",
    "\n",
    "        results.append(mean)\n",
    "\n",
    "    bundle_string = \"\"\n",
    "    for bundle in bundles:\n",
    "        bundle_string += bundle + \";\"\n",
    "    bundle_string = bundle_string[:-1]\n",
    "\n",
    "    np.savetxt(csv_file_out, np.array(results).transpose(), delimiter=\";\", header=bundle_string, comments=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_subjects_file(file_path):\n",
    "    with open(file_path) as f:\n",
    "        l = f.readline().strip()\n",
    "        if l.startswith(\"# tractometry_path=\"):\n",
    "            base_path = l.split(\"=\")[1]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid first line in subjects file. Must start with '# tractometry_path='\")\n",
    "\n",
    "        bundles = None\n",
    "        plot_3D_path = None\n",
    "\n",
    "        # parse bundle names\n",
    "        for i in range(2):\n",
    "            l = f.readline().strip()\n",
    "            if l.startswith(\"# bundles=\"):\n",
    "                bundles_string = l.split(\"=\")[1]\n",
    "                bundles = bundles_string.split(\" \")\n",
    "\n",
    "                valid_bundles = dataset_specific_utils.get_bundle_names(\"All_tractometry\")[1:]\n",
    "                for bundle in bundles:\n",
    "                    if bundle not in valid_bundles:\n",
    "                        raise ValueError(\"Invalid bundle name: {}\".format(bundle))\n",
    "\n",
    "                print(\"Using {} manually specified bundles.\".format(len(bundles)))\n",
    "            elif l.startswith(\"# plot_3D=\"):\n",
    "                plot_3D_path = l.split(\"=\")[1]\n",
    "\n",
    "        if bundles is None:\n",
    "            bundles = dataset_specific_utils.get_bundle_names(\"All_tractometry\")[1:]\n",
    "\n",
    "    df = pd.read_csv(file_path, sep=\" \", comment=\"#\")\n",
    "    df[\"subject_id\"] = df[\"subject_id\"].astype(str)\n",
    "\n",
    "    # Check that each column (except for first one) is correctly parsed as a number\n",
    "    for col in df.columns[1:]:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise IOError(\"Column {} contains non-numeric values\".format(col))\n",
    "\n",
    "    #if df.columns[1] == \"group\":\n",
    "    #    if df[\"group\"].max() > 1:\n",
    "    #        raise IOError(\"Column 'group' may only contain 0 and 1.\")\n",
    "\n",
    "    return base_path, df, bundles, plot_3D_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_for_confounds(values, meta_data, bundles, selected_bun_indices, NR_POINTS, analysis_type, confound_names):\n",
    "    values_cor = np.zeros([len(bundles), NR_POINTS, len(meta_data)])\n",
    "    for b_idx in selected_bun_indices:\n",
    "        for jdx in range(NR_POINTS):\n",
    "            target = np.array([values[s][b_idx][jdx] for s in meta_data[\"subject_id\"]])\n",
    "            if analysis_type == \"group\":\n",
    "                target_cor = metric_utils.unconfound(target, meta_data[[\"group\"] + confound_names].values,\n",
    "                                                     group_data=True)\n",
    "            else:\n",
    "                target_cor = metric_utils.unconfound(target, meta_data[confound_names].values,\n",
    "                                                     group_data=False)\n",
    "                meta_data[\"target\"] = metric_utils.unconfound(meta_data[\"target\"].values,\n",
    "                                                              meta_data[confound_names].values,\n",
    "                                                              group_data=False)\n",
    "            values_cor[b_idx, jdx, :] = target_cor\n",
    "\n",
    "    # Restore original data structure\n",
    "    values_cor = values_cor.transpose(2, 0, 1)\n",
    "    # todo: nicer way: use numpy array right from beginning instead of dict\n",
    "    values_cor_dict = {}\n",
    "    for idx, subject in enumerate(list(meta_data[\"subject_id\"])):\n",
    "        values_cor_dict[subject] = values_cor[idx]\n",
    "    return values_cor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corrected_alpha(values_allp, meta_data, analysis_type, subjects_A, subjects_B, alpha, bundles, nperm, b_idx):\n",
    "    if analysis_type == \"group\":\n",
    "        y = np.array((0,) * len(subjects_A) + (1,) * len(subjects_B))\n",
    "    else:\n",
    "        y = meta_data[\"target\"].values\n",
    "    alphaFWE, statFWE, clusterFWE, stats = AFQ_MultiCompCorrection(np.array(values_allp), y,\n",
    "                                                                   alpha, nperm=nperm)\n",
    "    #alphaFWE=0.05                                                               \n",
    "    print(\"Processing {}...\".format(bundles[b_idx]))\n",
    "    print(\"  cluster size: {}\".format(clusterFWE))\n",
    "    print(\"  alphaFWE: {}\".format(format_number(alphaFWE)))\n",
    "    return alphaFWE, clusterFWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_number(num):\n",
    "    if abs(num) > 0.00001:\n",
    "        return round(num, 6)\n",
    "    else:\n",
    "        return '%.2e' % Decimal(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_tractometry_with_pvalue(values, meta_data, bundles, selected_bundles, output_path, alpha, FWE_method,\n",
    "                                 analysis_type, correct_mult_tract_comp, show_detailed_p, nperm=1000,\n",
    "                                 hide_legend=False, plot_3D_path=None, plot_3D_type=\"none\",\n",
    "                                 tracking_format=\"trk_legacy\", tracking_dir=\"auto\", show_color_bar=True):\n",
    "\n",
    "    NR_POINTS = values[meta_data[\"subject_id\"][0]].shape[1]\n",
    "    selected_bun_indices = [bundles.index(b) for b in selected_bundles]\n",
    "\n",
    "    if analysis_type == \"group\":\n",
    "        subjects_A = list(meta_data[meta_data[\"group\"] == 0][\"subject_id\"])\n",
    "        subjects_B = list(meta_data[meta_data[\"group\"] == 1][\"subject_id\"])\n",
    "    else:\n",
    "        subjects_A = list(meta_data[\"subject_id\"])\n",
    "        subjects_B = []\n",
    "\n",
    "    confound_names = list(meta_data.columns[2:])\n",
    "\n",
    "    cols = 5\n",
    "    rows = math.ceil(len(selected_bundles) / cols)\n",
    "\n",
    "    a4_dims = (cols*3, rows*5)\n",
    "    f, axes = plt.subplots(rows, cols, figsize=a4_dims)\n",
    "\n",
    "    axes = axes.flatten()\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # Correct for confounds\n",
    "    values = correct_for_confounds(values, meta_data, bundles, selected_bun_indices, NR_POINTS, analysis_type,\n",
    "                                   confound_names)\n",
    "\n",
    "    # Significance testing with multiple correction of bundles\n",
    "    if correct_mult_tract_comp:\n",
    "        values_allp = []  # [subjects, NR_POINTS * nr_bundles]\n",
    "        for s in meta_data[\"subject_id\"]:\n",
    "            values_subject = []\n",
    "            for i, b_idx in enumerate(selected_bun_indices):\n",
    "                values_subject += list(values[s][b_idx]) # concatenate all bundles\n",
    "            values_allp.append(values_subject)\n",
    "        alphaFWE, clusterFWE = get_corrected_alpha(values_allp, meta_data, analysis_type, subjects_A, subjects_B, alpha,\n",
    "                                                   bundles, nperm, b_idx)\n",
    "\n",
    "    for i, b_idx in enumerate(tqdm(selected_bun_indices)):\n",
    "        # Bring data into right format for seaborn\n",
    "        data = {\"position\": [],\n",
    "                \"fa\": [],\n",
    "                \"group\": [],\n",
    "                \"subject\": []}\n",
    "        for j, subject in enumerate(subjects_A + subjects_B):\n",
    "            for position in range(NR_POINTS):\n",
    "                data[\"position\"].append(position)\n",
    "                data[\"subject\"].append(subject)\n",
    "                data[\"fa\"].append(values[subject][b_idx][position])\n",
    "                if subject in subjects_A:\n",
    "                    data[\"group\"].append(\"Group 0\")\n",
    "                else:\n",
    "                    data[\"group\"].append(\"Group 1\")\n",
    "\n",
    "        # Plot\n",
    "        ax = sns.lineplot(x=\"position\", y=\"fa\", data=data, ax=axes[i], hue=\"group\")\n",
    "                          # units=\"subject\", estimator=None, lw=1)  # each subject as single line\n",
    "\n",
    "        ax.set(xlabel='position along tract', ylabel='metric')\n",
    "        ax.set_title(bundles[b_idx])\n",
    "        if analysis_type == \"correlation\" or hide_legend:\n",
    "            ax.legend_.remove()\n",
    "        elif analysis_type == \"group\" and i > 0:\n",
    "            ax.legend_.remove()  # only show legend on first subplot\n",
    "\n",
    "\n",
    "        # Significance testing without multiple correction of bundles\n",
    "        if not correct_mult_tract_comp:\n",
    "            values_allp = [values[s][b_idx] for s in subjects_A + subjects_B]  # [subjects, NR_POINTS]\n",
    "            alphaFWE, clusterFWE = get_corrected_alpha(values_allp, meta_data, analysis_type, subjects_A, subjects_B,\n",
    "                                                       alpha, bundles, nperm, b_idx)\n",
    "\n",
    "        # Calc p-values\n",
    "        pvalues = np.zeros(NR_POINTS)\n",
    "        stats = np.zeros(NR_POINTS)  # for ttest: t-value, for pearson: correlation\n",
    "        for jdx in range(NR_POINTS):\n",
    "            if analysis_type == \"group\":\n",
    "                values_controls = [values[s][b_idx][jdx] for s in subjects_A]\n",
    "                values_patients = [values[s][b_idx][jdx] for s in subjects_B]\n",
    "                stats[jdx], pvalues[jdx] = scipy.stats.ttest_ind(values_controls, values_patients)\n",
    "            else:\n",
    "                values_controls = [values[s][b_idx][jdx] for s in subjects_A]\n",
    "                stats[jdx], pvalues[jdx] = scipy.stats.pearsonr(values_controls, meta_data[\"target\"].values)\n",
    "\n",
    "\n",
    "        # Plot significant areas\n",
    "        if show_detailed_p:\n",
    "            ax2 = axes[i].twinx()\n",
    "            ax2.bar(range(len(pvalues)), -np.log10(pvalues), color=\"gray\", edgecolor=\"none\", alpha=0.5)\n",
    "            ax2.plot([0, NR_POINTS-1], (-np.log10(alphaFWE),)*2, color=\"red\", linestyle=\":\")\n",
    "            ax2.set(xlabel='position', ylabel='-log10(p)')\n",
    "        else:\n",
    "            if FWE_method == \"alphaFWE\":\n",
    "                sig_areas = get_significant_areas(pvalues, 1, alphaFWE)\n",
    "            else:\n",
    "                sig_areas = get_significant_areas(pvalues, clusterFWE, alpha)\n",
    "            sig_areas = sig_areas * np.quantile(np.array(data[\"fa\"]), 0.98)\n",
    "            sig_areas[sig_areas == 0] = np.quantile(np.array(data[\"fa\"]), 0.02)\n",
    "            axes[i].plot(range(len(sig_areas)), sig_areas, color=\"red\", linestyle=\":\")\n",
    "\n",
    "        # Plot text\n",
    "        if FWE_method == \"alphaFWE\":\n",
    "            axes[i].annotate(\"alphaFWE:   {}\".format(format_number(alphaFWE)),\n",
    "                             (0, 0), (0, -35), xycoords='axes fraction', textcoords='offset points', va='top',\n",
    "                             fontsize=10)\n",
    "            axes[i].annotate(\"min p-value: {}\".format(format_number(pvalues.min())),\n",
    "                             (0, 0), (0, -45), xycoords='axes fraction', textcoords='offset points', va='top',\n",
    "                             fontsize=10)\n",
    "        else:\n",
    "            axes[i].annotate(\"clusterFWE:   {}\".format(clusterFWE),\n",
    "                             (0, 0), (0, -35), xycoords='axes fraction', textcoords='offset points', va='top',\n",
    "                             fontsize=10)\n",
    "\n",
    "        stats_label = \"t-value:      \" if analysis_type == \"group\" else \"corr.coeff.: \"\n",
    "        axes[i].annotate(stats_label + \"   {}\".format(format_number(stats[pvalues.argmin()])),\n",
    "                         (0, 0), (0, -55), xycoords='axes fraction', textcoords='offset points', va='top',\n",
    "                         fontsize=10)\n",
    "\n",
    "        if plot_3D_type != \"none\":\n",
    "\n",
    "            if plot_3D_type == \"metric\":\n",
    "                metric = np.array([values[s][b_idx] for s in subjects_A + subjects_B]).mean(axis=0)\n",
    "            else:\n",
    "                # metric = pvalues  # use this code if you want to plot the pvalues instead of the FA\n",
    "                metric = sig_areas\n",
    "\n",
    "            bundle = bundles[b_idx]\n",
    "            output_path_3D = output_path.split(\".\")[0] + \"_\" + bundle + \"_3D.png\"\n",
    "\n",
    "            if tracking_dir == \"auto\":\n",
    "                tracking_dir = tracking.get_tracking_folder_name(\"fixed_prob\", False)\n",
    "\n",
    "            if tracking_format == \"tck\":\n",
    "                tracking_path = join(plot_3D_path, tracking_dir, bundle + \".tck\")\n",
    "            else:\n",
    "                tracking_path = join(plot_3D_path, tracking_dir, bundle + \".trk\")\n",
    "            ending_path = join(plot_3D_path, \"endings_segmentations\", bundle + \"_b.nii.gz\")\n",
    "            mask_path = join(plot_3D_path, \"..\", \"nodif_brain_mask.nii.gz\")\n",
    "\n",
    "            if not os.path.isfile(tracking_path):\n",
    "                raise ValueError(\"Could not find: \" + tracking_path)\n",
    "            if not os.path.isfile(ending_path):\n",
    "                raise ValueError(\"Could not find: \" + ending_path)\n",
    "            if not os.path.isfile(mask_path):\n",
    "                raise ValueError(\"Could not find: \" + mask_path)\n",
    "\n",
    "            plot_utils.plot_bundles_with_metric(tracking_path, ending_path, mask_path, bundle, metric,\n",
    "                                                output_path_3D, tracking_format, show_color_bar)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse_subjects_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-56aad654ffaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnperm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcorrect_mult_tract_comp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_bundles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_3D_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_subjects_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/NAS/dumbo/protocoles/CogPhenoPark/Tractometry_MD.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0manalysis_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"group\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parse_subjects_file' is not defined"
     ]
    }
   ],
   "source": [
    "FWE_method = \"alphaFWE\"\n",
    "show_detailed_p = False\n",
    "hide_legend = False\n",
    "show_color_bar = True  # colorbar on 3D plot\n",
    "nperm = 5000\n",
    "nperm = int(nperm / 5)\n",
    "correct_mult_tract_comp = False\n",
    "base_path, meta_data, selected_bundles, plot_3D_path = parse_subjects_file(\"/NAS/dumbo/protocoles/CogPhenoPark/Tractometry_MD.txt\")\n",
    "analysis_type = \"group\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bundles = dataset_specific_utils.get_bundle_names(\"All_tractometry\")[1:]\n",
    "values = {}\n",
    "for subject in meta_data[\"subject_id\"]:\n",
    "    raw = np.loadtxt(base_path.replace(\"SUBJECT_ID\", subject), delimiter=\";\", skiprows=1).transpose()\n",
    "    values[subject] = raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AF_left',\n",
       " 'AF_right',\n",
       " 'ATR_left',\n",
       " 'ATR_right',\n",
       " 'CG_left',\n",
       " 'CG_right',\n",
       " 'CC_1',\n",
       " 'CC_2',\n",
       " 'CC_3',\n",
       " 'CC_4',\n",
       " 'CC_5',\n",
       " 'CC_6',\n",
       " 'CC_7',\n",
       " 'IFO_left',\n",
       " 'IFO_right',\n",
       " 'ILF_left',\n",
       " 'ILF_right',\n",
       " 'OR_left',\n",
       " 'OR_right',\n",
       " 'SLF_I_left',\n",
       " 'SLF_I_right',\n",
       " 'SLF_II_left',\n",
       " 'SLF_II_right',\n",
       " 'SLF_III_left',\n",
       " 'SLF_III_right',\n",
       " 'STR_left',\n",
       " 'STR_right',\n",
       " 'UF_left',\n",
       " 'UF_right',\n",
       " 'T_PREM_left',\n",
       " 'T_PREM_right',\n",
       " 'T_PAR_left',\n",
       " 'T_PAR_right',\n",
       " 'T_OCC_left',\n",
       " 'T_OCC_right',\n",
       " 'ST_FO_left',\n",
       " 'ST_FO_right',\n",
       " 'ST_PREM_left',\n",
       " 'ST_PREM_right']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/global/anaconda3/lib/python3.5/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ST_FO_left...\n",
      "  cluster size: 27.0\n",
      "  alphaFWE: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.28s/it]\n"
     ]
    }
   ],
   "source": [
    "plot_tractometry_with_pvalue(values, meta_data, all_bundles,selected_bundles,\"/NAS/dumbo/protocoles/CogPhenoPark/\",\n",
    "                             0.05, FWE_method, analysis_type, correct_mult_tract_comp,\n",
    "                             show_detailed_p, nperm=nperm, hide_legend=hide_legend,\n",
    "                             plot_3D_path=plot_3D_path, plot_3D_type=\"pval\",\n",
    "                             tracking_format=\"tck\", tracking_dir=\"auto\",\n",
    "                             show_color_bar=show_color_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/global/anaconda3/lib/python3.5/site-packages/matplotlib/pyplot.py:513: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]/home/global/anaconda3/lib/python3.5/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "  3%|▎         | 1/39 [00:04<02:44,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 2/39 [00:08<02:40,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 3/39 [00:12<02:35,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATR_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 4/39 [00:17<02:31,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATR_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 5/39 [00:21<02:26,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CG_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 6/39 [00:25<02:21,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CG_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 7/39 [00:30<02:17,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 8/39 [00:34<02:13,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 9/39 [00:38<02:08,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 10/39 [00:43<02:04,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 11/39 [00:47<02:00,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 12/39 [00:51<01:56,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 13/39 [00:55<01:51,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 14/39 [01:00<01:47,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IFO_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 15/39 [01:04<01:43,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IFO_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████      | 16/39 [01:08<01:38,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ILF_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▎     | 17/39 [01:13<01:34,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ILF_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 18/39 [01:17<01:30,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████▊     | 19/39 [01:21<01:26,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████▏    | 20/39 [01:26<01:21,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF_I_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 21/39 [01:30<01:17,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF_I_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▋    | 22/39 [01:34<01:13,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF_II_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▉    | 23/39 [01:38<01:08,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF_II_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 24/39 [01:43<01:04,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF_III_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 25/39 [01:47<01:00,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF_III_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 26/39 [01:51<00:55,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STR_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▉   | 27/39 [01:56<00:51,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STR_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 28/39 [02:00<00:47,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UF_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 29/39 [02:04<00:43,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UF_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 30/39 [02:09<00:39,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_PREM_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 31/39 [02:13<00:34,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_PREM_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 32/39 [02:17<00:30,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_PAR_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▍ | 33/39 [02:22<00:25,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_PAR_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 34/39 [02:26<00:21,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_OCC_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████▉ | 35/39 [02:30<00:17,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_OCC_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 36/39 [02:35<00:12,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ST_FO_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▍| 37/39 [02:39<00:08,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ST_FO_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 38/39 [02:43<00:04,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ST_PREM_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [02:48<00:00,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ST_PREM_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NR_POINTS = values[meta_data[\"subject_id\"][0]].shape[1]\n",
    "selected_bun_indices = [all_bundles.index(b) for b in selected_bundles]\n",
    "\n",
    "if analysis_type == \"group\":\n",
    "    subjects_A = list(meta_data[meta_data[\"group\"] == 0][\"subject_id\"])\n",
    "    subjects_B = list(meta_data[meta_data[\"group\"] == 1][\"subject_id\"])\n",
    "    subjects_C = list(meta_data[meta_data[\"group\"] == 2][\"subject_id\"])\n",
    "    subjects_D = list(meta_data[meta_data[\"group\"] == 3][\"subject_id\"])\n",
    "else:\n",
    "    subjects_A = list(meta_data[\"subject_id\"])\n",
    "    subjects_B = []\n",
    "\n",
    "confound_names = list(meta_data.columns[2:])\n",
    "\n",
    "cols = 3\n",
    "rows = math.ceil(len(selected_bundles) / cols)\n",
    "\n",
    "a4_dims = (cols*3, rows*5)\n",
    "f, axes = plt.subplots(rows, cols, figsize=a4_dims)\n",
    "\n",
    "axes = axes.flatten()\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Correct for confounds\n",
    "values = correct_for_confounds(values, meta_data, all_bundles, selected_bun_indices, NR_POINTS, analysis_type,confound_names)\n",
    "\n",
    "for i, b_idx in enumerate(tqdm(selected_bun_indices)):\n",
    "    \n",
    "    # Bring data into right format for seaborn\n",
    "    data = {\"position\": [],\n",
    "            \"fa\": [],\n",
    "            \"group\": [],\n",
    "            \"subject\": []}\n",
    "    for j, subject in enumerate(subjects_A + subjects_B + subjects_C + subjects_D ):\n",
    "        for position in range(NR_POINTS):\n",
    "            data[\"position\"].append(position)\n",
    "            data[\"subject\"].append(subject)\n",
    "            data[\"fa\"].append(values[subject][b_idx][position])\n",
    "            if subject in subjects_A:\n",
    "                data[\"group\"].append(\"Group 0\")\n",
    "            elif subject in subjects_B:\n",
    "                data[\"group\"].append(\"Group 1\")\n",
    "            elif subject in subjects_C:\n",
    "                data[\"group\"].append(\"Group 2\")                \n",
    "            else:\n",
    "                data[\"group\"].append(\"Group 3\")\n",
    "\n",
    "    # Plot\n",
    "    ax = sns.lineplot(x=\"position\", y=\"fa\", data=data, ax=axes[i], hue=\"group\")\n",
    "                      # units=\"subject\", estimator=None, lw=1)  # each subject as single line\n",
    "    print(all_bundles[b_idx])\n",
    "    ax.set(xlabel='position along tract', ylabel='metric')\n",
    "    ax.set_title(all_bundles[b_idx])\n",
    "    if analysis_type == \"correlation\" or hide_legend:\n",
    "        ax.legend_.remove()\n",
    "    elif analysis_type == \"group\" and i > 0:\n",
    "        ax.legend_.remove()  # only show legend on first subplot\n",
    "\n",
    "    alpha=0.05\n",
    "    nperm=1000\n",
    "    # Significance testing without multiple correction of bundles\n",
    "    if not correct_mult_tract_comp:\n",
    "        values_allp = [values[s][b_idx] for s in subjects_A + subjects_B + subjects_C + subjects_D ]  # [subjects, NR_POINTS]\n",
    "        #alphaFWE, clusterFWE = get_corrected_alpha(values_allp, meta_data, analysis_type, subjects_A, subjects_B,alpha,all_bundles, nperm, b_idx)\n",
    "        alphaFWE=0.05\n",
    "\n",
    "    # Calc p-values\n",
    "    pvalues = np.zeros(NR_POINTS)\n",
    "    stats = np.zeros(NR_POINTS)  # for ttest: t-value, for pearson: correlation\n",
    "    for jdx in range(NR_POINTS):\n",
    "        if analysis_type == \"group\":\n",
    "            values_A = [values[s][b_idx][jdx] for s in subjects_A]\n",
    "            values_B = [values[s][b_idx][jdx] for s in subjects_B]\n",
    "            values_C = [values[s][b_idx][jdx] for s in subjects_C]\n",
    "            values_D = [values[s][b_idx][jdx] for s in subjects_D]\n",
    "            stats[jdx], pvalues[jdx] = scipy.stats.kruskal(values_A, values_B,values_C, values_D)\n",
    "        else:\n",
    "            values_controls = [values[s][b_idx][jdx] for s in subjects_A]\n",
    "            stats[jdx], pvalues[jdx] = scipy.stats.pearsonr(values_controls, meta_data[\"target\"].values)\n",
    "\n",
    "    # Plot significant areas\n",
    "    if show_detailed_p:\n",
    "        ax2 = axes[i].twinx()\n",
    "        ax2.bar(range(len(pvalues)), -np.log10(pvalues), color=\"gray\", edgecolor=\"none\", alpha=0.5)\n",
    "        ax2.plot([0, NR_POINTS-1], (-np.log10(alphaFWE),)*2, color=\"red\", linestyle=\":\")\n",
    "        ax2.set(xlabel='position', ylabel='-log10(p)')\n",
    "    else:\n",
    "        if FWE_method == \"alphaFWE\":\n",
    "            sig_areas = get_significant_areas(pvalues, 1, alphaFWE)\n",
    "        else:\n",
    "            sig_areas = get_significant_areas(pvalues, clusterFWE, alpha)\n",
    "        sig_areas = sig_areas * np.quantile(np.array(data[\"fa\"]), 0.98)\n",
    "        sig_areas[sig_areas == 0] = np.quantile(np.array(data[\"fa\"]), 0.02)\n",
    "        axes[i].plot(range(len(sig_areas)), sig_areas, color=\"red\", linestyle=\":\")\n",
    "\n",
    "    # Plot text\n",
    "    if FWE_method == \"alphaFWE\":\n",
    "        axes[i].annotate(\"alphaFWE:   {}\".format(format_number(alphaFWE)),\n",
    "                         (0, 0), (0, -35), xycoords='axes fraction', textcoords='offset points', va='top',\n",
    "                         fontsize=10)\n",
    "        axes[i].annotate(\"min p-value: {}\".format(format_number(pvalues.min())),\n",
    "                         (0, 0), (0, -45), xycoords='axes fraction', textcoords='offset points', va='top',\n",
    "                         fontsize=10)\n",
    "    else:\n",
    "        axes[i].annotate(\"clusterFWE:   {}\".format(clusterFWE),\n",
    "                         (0, 0), (0, -35), xycoords='axes fraction', textcoords='offset points', va='top',\n",
    "                         fontsize=10)\n",
    "\n",
    "    stats_label = \"t-value:      \" if analysis_type == \"group\" else \"corr.coeff.: \"\n",
    "    axes[i].annotate(stats_label + \"   {}\".format(format_number(stats[pvalues.argmin()])),\n",
    "                     (0, 0), (0, -55), xycoords='axes fraction', textcoords='offset points', va='top',\n",
    "                     fontsize=10)\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/NAS/dumbo/protocoles/CogPhenoPark/GroupComparison_AllBundle_MD.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
